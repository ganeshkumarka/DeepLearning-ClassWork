{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b516031b",
   "metadata": {},
   "source": [
    "# Optimizer Theory and Mathematical Formulations\n",
    "\n",
    "## 1. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### Theory\n",
    "SGD is the fundamental optimization algorithm that updates parameters in the direction opposite to the gradient.\n",
    "\n",
    "### Mathematical Formulation\n",
    "```\n",
    "θ(t+1) = θ(t) - η * ∇L(θ(t))\n",
    "```\n",
    "Where:\n",
    "- θ(t): parameters at iteration t\n",
    "- η: learning rate\n",
    "- ∇L(θ(t)): gradient of loss function\n",
    "\n",
    "### Characteristics\n",
    "- **Pros**: Simple, memory efficient, can escape local minima due to noise\n",
    "- **Cons**: Oscillatory convergence, sensitive to learning rate, slow on ill-conditioned problems\n",
    "\n",
    "## 2. Adam (Adaptive Moment Estimation)\n",
    "\n",
    "### Theory\n",
    "Adam combines momentum with adaptive learning rates, maintaining running averages of both gradients and squared gradients.\n",
    "\n",
    "### Mathematical Formulation\n",
    "```\n",
    "m(t) = β₁ * m(t-1) + (1-β₁) * ∇L(θ(t))     [First moment]\n",
    "v(t) = β₂ * v(t-1) + (1-β₂) * (∇L(θ(t)))²  [Second moment]\n",
    "\n",
    "m̂(t) = m(t) / (1 - β₁ᵗ)  [Bias correction]\n",
    "v̂(t) = v(t) / (1 - β₂ᵗ)  [Bias correction]\n",
    "\n",
    "θ(t+1) = θ(t) - η * m̂(t) / (√v̂(t) + ε)\n",
    "```\n",
    "Where:\n",
    "- β₁ = 0.9 (momentum decay rate)\n",
    "- β₂ = 0.999 (squared gradient decay rate)\n",
    "- ε = 1e-8 (small constant for numerical stability)\n",
    "\n",
    "### Characteristics\n",
    "- **Pros**: Fast convergence, adaptive learning rates, robust to hyperparameters\n",
    "- **Cons**: May not converge to optimal solution in some cases, high memory usage\n",
    "\n",
    "## 3. RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "### Theory\n",
    "RMSprop adapts learning rates based on moving average of squared gradients, solving AdaGrad's diminishing learning rate problem.\n",
    "\n",
    "### Mathematical Formulation\n",
    "```\n",
    "v(t) = γ * v(t-1) + (1-γ) * (∇L(θ(t)))²\n",
    "θ(t+1) = θ(t) - η * ∇L(θ(t)) / (√v(t) + ε)\n",
    "```\n",
    "Where:\n",
    "- γ = 0.9 (decay rate)\n",
    "- ε = 1e-8 (small constant)\n",
    "\n",
    "### Characteristics\n",
    "- **Pros**: Adaptive learning rates, works well on non-stationary objectives\n",
    "- **Cons**: Still requires manual tuning of learning rate\n",
    "\n",
    "## 4. Adagrad (Adaptive Gradient Algorithm)\n",
    "\n",
    "\n",
    "### Theory\n",
    "Adagrad adapts learning rate based on historical gradients, giving frequently updated parameters smaller learning rates.\n",
    "\n",
    "### Mathematical Formulation\n",
    "```\n",
    "G(t) = G(t-1) + (∇L(θ(t)))²  [Cumulative squared gradients]\n",
    "θ(t+1) = θ(t) - η * ∇L(θ(t)) / (√G(t) + ε)\n",
    "```\n",
    "\n",
    "### Characteristics\n",
    "- **Pros**: No manual learning rate tuning, good for sparse features\n",
    "- **Cons**: Learning rate diminishes to zero, may stop learning prematurely\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d3c79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Results Analysis and Optimizer Selection\n",
    "\n",
    "## Expected Performance Patterns\n",
    "\n",
    "### Convergence Speed\n",
    "1. **Adam/Nadam**: Fastest initial convergence due to adaptive learning rates\n",
    "2. **RMSprop**: Good convergence, especially for RNNs\n",
    "3. **Adamax**: Stable convergence, may be slower than Adam\n",
    "4. **SGD**: Slower but steady convergence\n",
    "5. **Adagrad**: May slow down significantly in later epochs\n",
    "\n",
    "### Final Accuracy\n",
    "- **Adam/Nadam**: Usually achieve high accuracy quickly\n",
    "- **SGD**: May achieve good final accuracy but needs more epochs\n",
    "- **RMSprop**: Balanced performance\n",
    "- **Adagrad**: May plateau early due to diminishing learning rates\n",
    "\n",
    "## How to Choose the Right Optimizer\n",
    "\n",
    "### For Different Scenarios:\n",
    "\n",
    "1. **Computer Vision (CNNs)**:\n",
    "   - **Recommended**: Adam, SGD with momentum\n",
    "   - **Why**: Adam for quick prototyping, SGD for final tuning\n",
    "\n",
    "2. **Natural Language Processing (RNNs/Transformers)**:\n",
    "   - **Recommended**: Adam, RMSprop\n",
    "   - **Why**: Handle non-stationary objectives well\n",
    "\n",
    "3. **Large Datasets**:\n",
    "   - **Recommended**: SGD, Adam\n",
    "   - **Why**: Memory efficient, stable convergence\n",
    "\n",
    "4. **Small Datasets**:\n",
    "   - **Recommended**: Adam, RMSprop\n",
    "   - **Why**: Adaptive learning rates help with limited data\n",
    "\n",
    "### Selection Criteria:\n",
    "\n",
    "1. **Training Time Constraints**: Adam/Nadam for quick results\n",
    "2. **Memory Constraints**: SGD for minimal memory usage\n",
    "3. **Hyperparameter Sensitivity**: Adam for robustness\n",
    "4. **Final Performance**: SGD with proper tuning often achieves best results\n",
    "5. **Sparse Features**: Adagrad for sparse gradients\n",
    "\n",
    "## Hyperparameter Tuning Guidelines\n",
    "\n",
    "### Learning Rate Selection:\n",
    "- **SGD**: 0.01 - 0.1\n",
    "- **Adam**: 0.001 - 0.01\n",
    "- **RMSprop**: 0.001 - 0.01\n",
    "- **Adagrad**: 0.01 - 0.1\n",
    "\n",
    "### Batch Size Impact:\n",
    "- **Larger batches**: More stable gradients, better for SGD\n",
    "- **Smaller batches**: More noise, can help escape local minima\n",
    "- **Memory constraints**: Balance between performance and available memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3602e6",
   "metadata": {},
   "source": [
    "# Potential Viva Questions and Answers\n",
    "\n",
    "## Technical Implementation Questions\n",
    "\n",
    "### Q1: Why do we normalize the MNIST data by dividing by 255?\n",
    "**Answer**: \n",
    "- Pixel values range from 0-255 (8-bit integers)\n",
    "- Normalization to [0,1] helps with:\n",
    "  - Faster convergence (gradients don't explode/vanish)\n",
    "  - Numerical stability\n",
    "  - Consistent scale across features\n",
    "  - Better performance of activation functions\n",
    "\n",
    "### Q2: Why use different batch sizes for different optimizers?\n",
    "**Answer**:\n",
    "- **SGD**: Smaller batches (32) provide more frequent updates and noise\n",
    "- **Mini-batch SGD**: Larger batches (64) for more stable gradient estimates\n",
    "- **Adaptive optimizers (Adam, RMSprop)**: Standard batch size (32) works well\n",
    "- Trade-off between computational efficiency and gradient accuracy\n",
    "\n",
    "### Q3: Explain the choice of loss function and why it's suitable.\n",
    "**Answer**:\n",
    "- **Sparse Categorical Crossentropy**: Used because:\n",
    "  - Multi-class classification (10 classes)\n",
    "  - Labels are integers (0-9), not one-hot encoded\n",
    "  - Outputs probability distribution via softmax\n",
    "  - Mathematically: L = -log(p_true_class)\n",
    "\n",
    "## Theoretical Questions\n",
    "\n",
    "### Q4: What is the vanishing gradient problem and how do optimizers address it?\n",
    "**Answer**:\n",
    "- **Problem**: Gradients become very small in deep networks, slowing learning\n",
    "- **Solutions**:\n",
    "  - **Adam/RMSprop**: Adaptive learning rates scale gradients appropriately\n",
    "  - **Proper initialization**: Xavier/He initialization\n",
    "  - **Better activations**: ReLU instead of sigmoid/tanh\n",
    "\n",
    "### Q5: Explain the momentum concept in optimization.\n",
    "**Answer**:\n",
    "- **Concept**: Use weighted average of past gradients to smooth updates\n",
    "- **Benefits**:\n",
    "  - Accelerates convergence in consistent directions\n",
    "  - Dampens oscillations in valleys\n",
    "  - Helps escape local minima\n",
    "- **Mathematical**: v(t) = γ*v(t-1) + η*∇L, θ(t+1) = θ(t) - v(t)\n",
    "\n",
    "### Q6: Why might Adam fail to converge to the optimal solution?\n",
    "**Answer**:\n",
    "- **Exponential moving averages**: May not forget old gradients quickly enough\n",
    "- **Learning rate scheduling**: May need decay for fine-tuning\n",
    "- **Second moment estimation**: Can become too large, making updates too small\n",
    "- **Solution**: Use learning rate scheduling or switch to SGD for final epochs\n",
    "\n",
    "## Practical Questions\n",
    "\n",
    "### Q7: How would you modify this code for a regression problem?\n",
    "**Answer**:\n",
    "- **Output layer**: Single neuron, linear activation\n",
    "- **Loss function**: Mean Squared Error (MSE) or Mean Absolute Error (MAE)\n",
    "- **Metrics**: MSE, MAE, R²\n",
    "- **Data preprocessing**: StandardScaler for targets\n",
    "\n",
    "### Q8: What if training accuracy is high but validation accuracy is low?\n",
    "**Answer**:\n",
    "- **Problem**: Overfitting\n",
    "- **Solutions**:\n",
    "  - Add dropout layers\n",
    "  - Reduce model complexity\n",
    "  - Add L1/L2 regularization\n",
    "  - Increase training data\n",
    "  - Early stopping\n",
    "  - Data augmentation\n",
    "\n",
    "### Q9: How would you implement early stopping?\n",
    "**Answer**:\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(..., validation_split=0.2, callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "## Advanced Questions\n",
    "\n",
    "### Q10: Explain the bias correction in Adam optimizer.\n",
    "**Answer**:\n",
    "- **Problem**: Initial moments are biased toward zero\n",
    "- **Solution**: Divide by (1 - β^t) where t is iteration number\n",
    "- **Effect**: Larger effective learning rate in early iterations\n",
    "- **Mathematical**: m̂ = m/(1-β₁^t), v̂ = v/(1-β₂^t)\n",
    "\n",
    "### Q11: When would you use learning rate scheduling and how?\n",
    "**Answer**:\n",
    "- **When**: Long training, fine-tuning, avoiding overshooting minima\n",
    "- **Methods**:\n",
    "  - Step decay: Reduce by factor every N epochs\n",
    "  - Exponential decay: Continuous exponential reduction\n",
    "  - Cosine annealing: Smooth reduction following cosine function\n",
    "  - Adaptive: Based on validation loss plateau\n",
    "\n",
    "### Q12: How do you handle class imbalance in this dataset?\n",
    "**Answer**:\n",
    "- **Check distribution**: Plot class frequencies\n",
    "- **Solutions**:\n",
    "  - Weighted loss function: class_weight parameter\n",
    "  - Oversampling: SMOTE, random oversampling\n",
    "  - Undersampling: Random undersampling\n",
    "  - Focal loss: Focus on hard examples\n",
    "  - Stratified sampling: Ensure balanced batches"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
